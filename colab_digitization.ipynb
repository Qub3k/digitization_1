{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"colab_digitization.ipynb","provenance":[{"file_id":"https://github.com/gpasterczyk/digitization_1/blob/main/wstkt.ipynb","timestamp":1612046025474}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"CpUypy8i646y"},"source":["# Basics of Digitization\r\n","*Mikołaj Leszczuk, Jakub Nawała, Grzegorz Pasterczyk*\r\n"]},{"cell_type":"markdown","metadata":{"id":"0SzvoqvqwbVy"},"source":["## Introduction\r\n","One of the basic tasks in processing and transmitting video signals is signal conversion. In particular, signals are converted from the analogue to digital form. Sometimes, at the same processing stage, visual material acquired is compressed “on-the-fly”. Then it is stored using one of the popular container formats like MP4 or AVI. This fast, “on-the-fly” compression may not be sufficient for some applications. Thus, further recompression is sometimes necessary (e.g., to fit into predefined streaming bandwidth or satisfy maximum file size requirements).<br>\r\n","Viewing (i.e., playing) a compressed video requires decompression (most often in real time). Afterwards, moving pictures are transmitted onto a screen (in a digital or, after a conversion, in an analogue form). The processes of digitization and viewing will be performed throughout this exercise."]},{"cell_type":"markdown","metadata":{"id":"aFVfHxq27CvQ"},"source":["## Purpose of the Exercise \r\n","The purpose of this exercise is to larn about digitization of analogue video signals (for example, as recorded with your built-in webcam)\r\n","\r\n","Simultaneously, you will learn how to play-out the previously created file. For digitization purposes, the built-in webcam of your computer and this Jupyter Notebook will be used."]},{"cell_type":"markdown","metadata":{"id":"8kINU4Dy7KGw"},"source":["## Execution of the Exercise\r\n","Please read and execute the whole instruction. The exercise stand should be equipped with a computer having a monitor, camera and an internet connection."]},{"cell_type":"markdown","metadata":{"id":"T0FkMldh7XLG"},"source":["## Setting Up the Computer\r\n","The first time you use this notebook you will be asked to allow it to access your camera.\r\n","Please allow the notebook to access the camera to make sure everything works as expected."]},{"cell_type":"code","metadata":{"id":"5IyftoBUlS0E"},"source":["#@title <=== Please click and wait several seconds to prepare the software environment { display-mode: \"form\" }\r\n","!apt-get install libzbar0\r\n","!pip3 install pyzbar\r\n","import matplotlib.pyplot as pyplot\r\n","import cv2\r\n","import numpy as np\r\n","from ipywidgets import interact, IntSlider\r\n","import pyzbar.pyzbar as pyzbar\r\n","import urllib.request, urllib.error\r\n","import os.path\r\n","from google.colab import files\r\n","!git clone https://github.com/gpasterczyk/digitization_1\r\n","\r\n","def readImage(path):\r\n","    if os.path.isfile(path):\r\n","        cap = cv2.imread(path)\r\n","        oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\r\n","        return oryg\r\n","\r\n","    else:\r\n","        try:\r\n","            response = urllib.request.urlopen(path)\r\n","            code = 200\r\n","        except urllib.error.URLError as e:\r\n","            code = 'Name resolve problem'\r\n","        except urllib.error.HTTPError as e:\r\n","            code = e.code\r\n","        if code == 200:\r\n","            req = urllib.request.urlopen(urllib.request.Request(path, headers={'User-Agent': 'Mozilla/82.0'}))\r\n","            arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\r\n","            cap = cv2.imdecode(arr, -1)\r\n","            oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\r\n","            return oryg\r\n","        else:\r\n","            print(code)\r\n","            #cap = cv2.imread('digitization_1/files/http_error.png')\r\n","            #oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\r\n","            #return oryg\r\n","        print(code)\r\n","\r\n","def ploting2(toPlot1, title1, toPlot2, title2, cmap):\r\n","    pyplot.figure(figsize=[15.0, 5.0])\r\n","    pyplot.subplot(1,2,1)\r\n","    pyplot.title(title1)\r\n","    pyplot.imshow(toPlot1)\r\n","    pyplot.axis('off')\r\n","    pyplot.subplot(1,2,2)\r\n","    pyplot.title(title2)\r\n","    pyplot.imshow(toPlot2, cmap)\r\n","    pyplot.axis('off')\r\n","    pyplot.show()\r\n","\r\n","def cs(image, set_channel):\r\n","    oryg = readImage(image)\r\n","    title1 = 'Original image'\r\n","    title2 = 'Channel {} of original image'.format(set_channel)\r\n","    cmap = 'viridis'\r\n","\r\n","    if set_channel in {'R','G','B'}:\r\n","        h, w = oryg.shape[0], oryg.shape[1]\r\n","        zeros = np.zeros((h,w), dtype=\"uint8\")\r\n","        r, g, b = cv2.split(oryg)\r\n","        if set_channel == 'R':\r\n","            typ = cv2.merge((r,zeros,zeros))\r\n","        elif set_channel == 'G':\r\n","            typ = cv2.merge((zeros,g,zeros))\r\n","        elif set_channel == 'B':\r\n","            typ = cv2.merge((zeros,zeros,b))\r\n","\r\n","\r\n","    elif set_channel in {'Y','Cr','Cb'}:\r\n","        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YCrCb)\r\n","        cmap = 'gray'\r\n","        if set_channel == 'Y':\r\n","            typ = cv2.split(frame)[0]\r\n","        elif set_channel == 'Cb':\r\n","            typ = cv2.split(frame)[2]\r\n","        elif set_channel == 'Cr':\r\n","            typ = cv2.split(frame)[1]\r\n","\r\n","\r\n","    elif set_channel in {'Y_','U','V'}:\r\n","        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YUV)\r\n","        cmap = 'gray'\r\n","        if set_channel == 'Y_':\r\n","            typ = cv2.split(frame)[0]\r\n","        elif set_channel == 'U':\r\n","            typ = cv2.split(frame)[1]\r\n","        elif set_channel == 'V':\r\n","            typ = cv2.split(frame)[2]\r\n","\r\n","\r\n","    elif set_channel in {'H','S','V_'}:\r\n","        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2HSV)\r\n","        cmap = 'gray'\r\n","        if set_channel == 'H':\r\n","            typ = cv2.split(frame)[0]\r\n","        elif set_channel == 'S':\r\n","            typ = cv2.split(frame)[1]\r\n","        elif set_channel == 'V_':\r\n","            typ = cv2.split(frame)[2]\r\n","\r\n","\r\n","    elif set_channel in {'RGB','YCbCr','YUV','HSV'}:\r\n","        title2 = 'Original image in {} color space'.format(set_channel)\r\n","        if set_channel == 'RGB':\r\n","            typ = oryg\r\n","        elif set_channel == 'YCbCr':\r\n","            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YCrCb)\r\n","            cap = cv2.cvtColor(frame, cv2.COLOR_YCrCb2RGB)\r\n","            typ = cap\r\n","        elif set_channel == 'YUV':\r\n","            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YUV)\r\n","            cap = cv2.cvtColor(frame, cv2.COLOR_YUV2RGB)\r\n","            typ = cap\r\n","        elif set_channel == 'HSV':\r\n","            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2HSV)\r\n","            cap = cv2.cvtColor(frame, cv2.COLOR_HSV2RGB)\r\n","            typ = cap\r\n","    ploting2(oryg, title1, typ, title2, cmap)\r\n","\r\n","def histogram(image):\r\n","    oryg = readImage(image)\r\n","    h, w = oryg.shape[0], oryg.shape[1]\r\n","    print('Loaded image size is {}x{}'.format(w, h))\r\n","    color = ('r','g','b')\r\n","    pyplot.figure(figsize=[15.0, 5.0])\r\n","    pyplot.subplot(1,2,1)\r\n","    pyplot.title('Original image')\r\n","    pyplot.axis('off')\r\n","    pyplot.imshow(oryg)\r\n","    pyplot.subplot(1,2,2)\r\n","    pyplot.xlabel('Pixels value')\r\n","    pyplot.ylabel('Number of pixels')\r\n","    pyplot.title('Histogram of original image')\r\n","    for i,color in enumerate(color):\r\n","        hist = cv2.calcHist([oryg],[i],None,[256],[0,256])\r\n","        pyplot.plot(hist,color = color)\r\n","        pyplot.xlim([0,256])\r\n","    pyplot.show()\r\n","\r\n","def edges(image):\r\n","    oryg = readImage(image)\r\n","    edges = cv2.Canny(oryg,100,200)\r\n","    title1 = 'Original image'\r\n","    title2 = 'Canny edge detecting image output'\r\n","    cmap = 'viridis'\r\n","    ploting2(oryg,title1,edges,title2,cmap)\r\n","\r\n","def scanCodes(image):\r\n","    oryg = readImage(image)\r\n","    image = oryg.copy()\r\n","    decodedObjects = pyzbar.decode(oryg)\r\n","    if decodedObjects != '':\r\n","        for obj in decodedObjects:\r\n","            print('Type of code : ', obj.type)\r\n","            print('Data on code: ', obj.data,'\\n')\r\n","    else:\r\n","      print('Unable to read code. Please try another image.')\r\n","    for decodedObject in decodedObjects:\r\n","        points = decodedObject.polygon\r\n","        if len(points) > 4 :\r\n","            hull = cv2.convexHull(np.array([point for point in points]))#, dtype=np.float32))\r\n","            hull = list(map(tuple, np.squeeze(hull)))\r\n","        else :\r\n","            hull = points;\r\n","\r\n","        n = len(hull)\r\n","\r\n","        for j in range(0,n):\r\n","            cv2.line(image, hull[j], hull[ (j+1) % n], (255,0,0), 3)\r\n","    title1 = 'Original image'\r\n","    title2 = 'Image with surrounded barcode'\r\n","    cmap = 'viridis'\r\n","    ploting2(oryg,title1,image,title2,cmap)\r\n","\r\n","def make_file(select_video,frame_rate):\r\n","    path = 'digitization_1/files/{}.mp4'.format(select_video)\r\n","    cap = cv2.VideoCapture(path)\r\n","    size = int(cap.get(3)),int(cap.get(4))\r\n","    fourcc = cv2.VideoWriter_fourcc(*'VP08')\r\n","    out = cv2.VideoWriter('frame_rates.webm', fourcc, frame_rate, size)\r\n","    while(cap.isOpened()):\r\n","        ret, frame = cap.read()\r\n","        if ret==True:\r\n","            out.write(frame)\r\n","        else:\r\n","            break\r\n","    files.download('frame_rates.webm')\r\n","\r\n","##### Ćwiczenie z HDR\r\n","\r\n","def show_oryg(img_1, img_2, img_3):\r\n","\r\n","    pyplot.figure(figsize=[12.0, 8.0])\r\n","    pyplot.subplot(1,3,1)\r\n","    pyplot.imshow(img_1)\r\n","    pyplot.axis('off')\r\n","    pyplot.subplot(1,3,2)\r\n","    pyplot.imshow(img_2)\r\n","    pyplot.axis('off')\r\n","    pyplot.subplot(1,3,3)\r\n","    pyplot.imshow(img_3)\r\n","    pyplot.axis('off')\r\n","    pyplot.suptitle('Original taken images', x=0.5, y=0.65, fontsize=21)\r\n","\r\n","def plotingHDR(hdr_out, hdr_type):\r\n","    pyplot.figure(figsize=[7.5, 5.0])\r\n","    pyplot.title('Image created by {} HDR type.'.format(hdr_type.upper()))\r\n","    pyplot.imshow(hdr_out)\r\n","    pyplot.axis('off')\r\n","    pyplot.show()\r\n","\r\n","def loadFilesHDR(image):\r\n","    img_fn = ['digitization_1/files/{}{}.png'.format(image,str(num)) for num in range(1,4)]\r\n","    img_list = [cv2.imread(fn) for fn in img_fn]\r\n","    img_list_ok = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in img_list]\r\n","    return img_list_ok;\r\n","\r\n","def hdr(image, hdr_type):\r\n","    img_list_ok = loadFilesHDR(image)\r\n","    img_1, img_2, img_3 = img_list_ok\r\n","    show_oryg(img_1, img_2, img_3)\r\n","    exposure_times = np.array([50.0, 11.0, 3.0],dtype=np.float32) #, dtype=np.float32\r\n","    tonemap1 = cv2.createTonemap(gamma=2.5)\r\n","\r\n","    if hdr_type == 'robertson':\r\n","        merge_robertson = cv2.createMergeRobertson()\r\n","        hdr_robertson = merge_robertson.process(img_list_ok, times=exposure_times.copy())\r\n","        res_robertson = tonemap1.process(hdr_robertson.copy())\r\n","        hdr_out = np.clip(res_robertson*255, 0, 255).astype('uint8')\r\n","\r\n","\r\n","    elif hdr_type == 'mertens':\r\n","        merge_mertens = cv2.createMergeMertens()\r\n","        res_mertens = merge_mertens.process(img_list_ok)\r\n","        hdr_out = np.clip(res_mertens*255, 0, 255).astype('uint8')\r\n","\r\n","    elif hdr_type == 'debevec':\r\n","        time = exposure_times\r\n","        merge_debevec = cv2.createMergeDebevec()\r\n","        hdr_debevec = merge_debevec.process(img_list_ok, times=time.copy())\r\n","        res_debevec = tonemap1.process(hdr_debevec.copy())\r\n","        hdr_out = np.clip(res_debevec*255, 0, 255).astype('uint8')\r\n","\r\n","    else:\r\n","        print('Błąd funkcji \"hdr\".')\r\n","\r\n","    plotingHDR(hdr_out, hdr_type)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IiubEhglkDe_"},"source":["## Experimenting with frame rates\n","Please try to change the default frame rate.<br>\n","\n","<font color=red><b>Mention that the dropdown menu will be visible only after the appropriate code cell is run</b></font>\n","\n","From dropdown menu choose one of example videos and set number of frames per second on slide bar. A video file will be downloaded automatically. Please open it and observe changes. Each change will generate another file.<br><br>"]},{"cell_type":"code","metadata":{"id":"pTAiLlOD7Hho"},"source":["#@title Exercise { display-mode: \"form\" }\r\n","interact(make_file, select_video = [('Falling fruit','lemon'),('View on street','cariage'),('Reading people','glasses')], frame_rate = IntSlider(min=5, max=150, step=5, continuous_update=False));\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyFNrzgIkDfK"},"source":["## Experiment With Color Spaces\n","\n","Please try to change the default colour space for loaded image. Please observe the relation between the image colours and colour space.<br>\n","\n","At first click inside code-block and click Run button in navigate panel.\n","\n","<font color=red><b>What do you mean by \"advantage red colour\"?</b></font><br>\n","<font color=blue><b>Chciałem zaznaczyć, że na tym zdjęciu przeważa kolor czerwony (w kanale RED jest najwięcej pikseli o wysokich wartościach)</b></font>"]},{"cell_type":"code","metadata":{"id":"GJ0B5ioUkDfL"},"source":["#@title Exercise 1 - run cell and experiment with prepared images { display-mode: \"form\" }\r\n","interact(cs, image=[('Advantage red color', 'digitization_1/files/red.jpg'),('Advantage green color', 'digitization_1/files/green.jpg'),('Advantage blue color', 'digitization_1/files/blue.jpg'), ('Random shapes', 'digitization_1/files/rgb.jpg')], set_channel=[('RGB color space','RGB'),('R','R'),('G','G'),('B','B'),('YCbCr color space','YCbCr'),('Y','Y'),('Cb','Cb'),('Cr','Cr'),('YUV color space','YUV'),('Y','Y_'),('U','U'),('V','V'),('HSV','HSV'),('H','H'),('S','S'),('V','V_')]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCH9-6jUkDfL"},"source":["#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\r\n","interact(cs, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353569.jpg', set_channel=[('RGB color space','RGB'),('R','R'),('G','G'),('B','B'),('YCbCr color space','YCbCr'),('Y','Y'),('Cb','Cb'),('Cr','Cr'),('YUV color space','YUV'),('Y','Y_'),('U','U'),('V','V'),('HSV','HSV'),('H','H'),('S','S'),('V','V_')]);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxZGd6ZwkDfM"},"source":["## Experimenting With Picture Histogram\n","This demo uses the 2-D Histogram in Image Processing to calculate the histograms of R, G, and B values.\n","\n","Click inside code-block and click Run button in navigate panel."]},{"cell_type":"code","metadata":{"id":"mvYBxlJ5kDfM"},"source":["#@title Exercise 1 - experiment with pre-loaded images { display-mode: \"form\" }\r\n","interact(histogram, image=[('Advantage red color', 'digitization_1/files/red.jpg'),('Advantage green color', 'digitization_1/files/green.jpg'),('Advantage blue color', 'digitization_1/files/blue.jpg'),('Overexposure image','digitization_1/files/overexp.jpg'),('Underexposure image','digitization_1/files/underexp.jpg')]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGjvGOPtkDfN"},"source":["#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\r\n","interact(histogram, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353585.jpg');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnOL7G7hkDfO"},"source":["## Experimenting with Edge Detection\n","The Canny method is applied to find the edges of objects in the input images\n","\n","Click inside code-block and click Run button in navigate panel."]},{"cell_type":"code","metadata":{"id":"WsSEEJ7bkDfO"},"source":["#@title Exercise 1 - run cell and experiment with pre-loaded images { display-mode: \"form\" }\r\n","interact(edges, image=[('Random shapes', 'digitization_1/files/shapes.jpg'),('Advantage red color', 'digitization_1/files/red.jpg'),('Advantage green color', 'digitization_1/files/green.jpg'),('Advantage blue color', 'digitization_1/files/blue.jpg')]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NC_HOOWGkDfP"},"source":["#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\r\n","interact(edges, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353532.jpg');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AyxMt9VOkDfP"},"source":["## Experimenting with Barcode Recognition"]},{"cell_type":"code","metadata":{"id":"fePzFGIikDfQ"},"source":["#@title Exercise 1 - Experiment with pre-loaded images { display-mode: \"form\" }\r\n","interact(scanCodes, image=[('QR 1','digitization_1/files/qrcode1.jpg'),('QR 2','digitization_1/files/qrcode2.jpg'),('QR 3','digitization_1/files/qrcode3.jpg'),('BAR 1','digitization_1/files/barcode1.jpg'),('BAR 2','digitization_1/files/barcode2.jpg'),('BAR 3','digitization_1/files/barcode3.jpg')]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_f_rhdRkDfR"},"source":["#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\r\n","interact(scanCodes, image='https://barcodesgonewild.com/wp-content/uploads/2014/05/Axe-barcode-1.jpeg');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2ZaNTF6kDfR"},"source":["# Digitization - Advanced Topics in Digital Photography\r\n","*Michał Grega, Mikołaj Leszczuk, Jakub Nawała, Grzegorz Pasterczyk*"]},{"cell_type":"markdown","metadata":{"id":"FcVVyiNQNltq"},"source":["## Purpose \r\n","Purpose of this laboratory is to present the RAW processing workflow for digital images and the HDR technique."]},{"cell_type":"markdown","metadata":{"id":"FzVguLU1NqPr"},"source":["## Prerequisites\r\n","* Basics of digital photography\r\n","* Basics of image formats"]},{"cell_type":"markdown","metadata":{"id":"l8J5iCPQN9ms"},"source":["## RAW Processing\r\n","*All photographs are © Michał Grega unless stated otherwise.*"]},{"cell_type":"markdown","metadata":{"id":"LDoMDSLnOCSK"},"source":["## What is RAW and why to use it?\r\n","RAW is a file format used for storing the information on the image taken by the digital camera. It is not an image format. It contains raw (unprocessed) data stored by the physical sensor (radiometric data). Apart from that, a RAW file may contain additional metadata on:\r\n","* Make and model of the camera,\r\n","* Physical properties of the sensor,\r\n","* Exposure and camera settings,\r\n","* Lens settings,\r\n","* A highly compressed .jpg thumbnail of the image.\r\n","\r\n","Please note, that the camera sensor is most commonly not a pixel matrix (see Fig. 1). It is a CMOS or a CCD (charge-coupled device) sensor covered by a filter (see Fig. 2). Underneath this filter, there is an array of photosensitive subpixels, which do not have to be even of rectangular shape. Therefore in order to convert the radiometric data to an image detailed information on the sensor geometry must be available for the software algorithm.\r\n","\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1ptl8YS1g5EqjQ7_Y0oK3WL1naqTKkOEf\" width=\"500\">\r\n","\r\n","Fig. 1. Sensor layouts (Wikipedia)\r\n","\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1O_Y89x3GTTPQ9J8Qqte8In9-DQ4cC43y\" width=\"500\">\r\n","\r\n","Fig. 2. Bayer colour filter (do you see anything unexpected?) (Wikipedia)</center></div></body></html>"]},{"cell_type":"markdown","metadata":{"id":"AuWT40wsOHf3"},"source":["## How it differs from .jpg or .tiff?\r\n","A RAW image captured by a camera is an uncompressed and unprocessed raw measurement of light. It is commonly referred to as a digital negative, as it serves a similar purpose as a traditional film negative. A .jpg or .tiff image produced by the camera is that digital negative processed (developed) on the fly by the camera built-in software. The software most typically conducts a set of automated operations:\r\n","1. develop the raw image (knowing the physical properties of the sensor),\r\n","* enhance the resulting image (by applying contrast and colour correction and sharpening),\r\n","* apply additional correction algorithms (e.g. red-eye reduction),\r\n","* compress the image to the desired format (lossy .jpg or lossless .tiff)."]},{"cell_type":"markdown","metadata":{"id":"FbFbOWFmOOsd"},"source":["## What are the benefits of RAW shooting?\r\n","The most profound and important benefit is that a photographer retains full control of the creation, correction and compression processes. All the adjustments can be made by hand and tuned in order to achieve the desired effect.<br> \r\n","Moreover, RAW files offer much better input for post-processing, as the state-of-the-art sensors store (digitize) the data at 14 bits per colour per pixel. It means that a raw image can hold 214 shades per colour, meaning 242 total colours. A .jpg file typically saves 8 bits per colour, meaning 28 shades per colour resulting in 224 colours. In short, the RAW format offers better colour fidelity **(18 orders of magnitude greater than .jpg)**, much higher dynamic range (High Dynamic Range imaging will be explained further on) and more data for further corrections.<br>\r\n","What are the drawbacks of RAW shooting?\r\n","* The visual quality of an unenhanced RAW file is not satisfying as no corrections are applied,\r\n","* RAW images are of large size,\r\n","* RAW images require tedious manual development and correction. "]},{"cell_type":"markdown","metadata":{"id":"daWS7AyNOVXQ"},"source":["## RAW processing workflow\r\n","`\r\n","Disclaimer:\r\n","Photography is an art and thus slips away from scientific definition. Moreover, it is controversial how much post-processing (a.k.a) “photoshopping” is allowed to a professional. Photographic agencies and photographic competitions have strict rules that define what is allowed and what is not.\r\n","`\r\n","The RAW processing workflow consists of several steps - all described below. Each photographer usually creates his/her own workflow by adding or removing some of the steps. It is important to sustain the order of the steps, as there is a logic behind them (e.g. sharpening has to be done prior to development).\r\n","1. **Cropping and straightening** – a selection of a composition of the image\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=18wRyA5FDFnTovss-rfxsa_0ae3M0YICC\" width=\"500\"/>\r\n","<p>Before and after straightening </p></center></div></body></html>\r\n","\r\n","2. **Exposure correction** – done in order to correct for over- or underexposed images. Due to the physical characteristics of the sensor a rule of the thumb is that it is better to shoot under-. rather than, overexposed images as it is easier to compensate for underexposure. The most useful tool is the luminosity histogram. A well-exposed photo covers the whole dynamic range and fills the whole histogram. An under- or overexposed photo shows clipping in (respectively) low or high values.\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1mtBmPzGXFJ2bZQIxpNljrwYL15BOAePe\" width=\"500\"/>\r\n","<p>A histogram of an underexposed image </p>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=189mjDgxRqx2B96T7HwAYOishQ5rZAwme\" width=\"500\"/>\r\n","<p>A histogram of an overexposed image</p>\r\n","<p>(gray area - total luminosity; red, green and blue curves - luminosity for each RGB channel)</p>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1i8B-2KZ4hwshZPAJXDEuDU5zA0FMCX-Z\" width=\"500\"/>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1ZqMOJaDyymCdckE8OsBKXco0pK-uf6Aj\" width=\"500\"/>\r\n","<p>Before and after exposure correction. Notice the histogram.</p>\r\n","</center></div></body></html>\r\n","\r\n","More advanced software allows for a software-based increase in dynamic range (i.e. the increase in an end-to-end distance between extreme pixel values). Software algorithm detects the under- or overexposed parts of the image and enhances them instead of modifying the whole image.\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1dChZqeL3rcYU_KuTXov6sJqQ6_z-esvv\" width=\"600\"/>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1EDdxReNJgOr6fWlnY2PlMAwDCpiooSxB\" width=\"600\"/>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1ngDySajgzyMw0a7jdLrSidXeevu-Vky2\" width=\"600\"/>\r\n","<p> Exposure correction using overall exposure and software HDR. Notice the contrast between foreground and background.</p>\r\n","</center></div></body></html>\r\n","\r\n","3. **Contrast correction** – increases the contrast in an image. Images captured in the RAW format appear to be flat and not vibrant. That is due to the lack of contrast correction. Contrast is the difference between the brightest and darkest pixel in the image. While it is easy to define, there are many algorithms that aim at improving contrast by maintaining the general luminosity and colours of the image. \r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1P2nc32N7ok7vxQhGKGhJ976SJsJj8dat\" width=\"600\"/>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1GWIqYDuiVI9sE_nPkkCig6qnuSTGX-zX\" width=\"600\"/>\r\n","<p> Contrast correction </p>\r\n","</center></div></body></html>\r\n","\r\n","4. **Colour correction** – shooting an image in given conditions may cause the colours to be distorted. Especially the type of light (sunlight vs artificial) makes the colours unnatural. For example, shooting in artificial incandescent light causes images to be unnaturally warm (due to the high amount of infrared radiation). On the other hand, shooting in full sunlight on high altitudes causes photographs to be unnaturally cool (because of the high amount of UV radiation). It can be compensated for using white balance compensation.\r\n","\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1VdqrzX6ojjP5w0xrDm3l109Is9bv9ASu\" width=\"600\"/>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1lVOL2GqRPdqUK3pBkoilNPkXJ69KKSCs\" width=\"600\"/>\r\n","<p> White balance compensation – notice the clipping on a histogram in the red channel. </p>\r\n","</center></div></body></html>\r\n","\r\n","5. **Sharpening and detail** – allows to sharpen the image and remove unnecessary artefacts. Among those are spots caused by dirt on the sensor (or lenses) and noise generated by the sensor itself.\r\n","\r\n","<html><body><div><center>\r\n","<img src=\"https://drive.google.com/uc?export=view&id=1xKAT3EMW0kE0TfePj4p6Ty_g7izh-kx6\" width=\"600\"/>\r\n","<p> Sharpening and noise reduction </p>\r\n","</center></div></body></html>\r\n","\r\n","6. **Development** – allows converting the image to the target format and colour space.\r\n"]},{"cell_type":"markdown","metadata":{"id":"-9ozYL_PO4qo"},"source":["## RAW processing exercise\r\n","1. You can use any RAW processing software you wish. Note, however, that the paid software usually offers a more intuitive interface and more advanced algorithms. If you own a DSLR (Digital Single-Lens Reflex) camera you probably got a copy of the manufacturer’s software. Other (costly) solutions are Adobe Photoshop with Lightroom or Capture One (for the use in the laboratory you have to download a version from https://www.phaseone.com/en/Download.aspx).<br>You can also use (free) http://rawtherapee.com/.<br>Examples shown in the previous section were prepared using Capture One Pro, which offers a free 30-day trial.\r\n","But also you can see exercise below to see how RAW processing works on 3 examples.\r\n","2. Download example RAW files (see the “RAW Examples” folder accompanying this instruction).\r\n","3. Develop these RAW files into *.jpg images for web publishing trying to achieve the best visual result. Correct the composition, exposure and colours of the image. Apply sharpening and the correct developmental recipe. Observe what happens when you use high values of the corrections for sharpness, software HDR, exposition. There is a saying for beginners “Set up your sliders in a position that makes your photograph look good and then reduce all by half”."]},{"cell_type":"markdown","metadata":{"id":"veqepdyeO76B"},"source":["## HDR Imaging\r\n","As you might have noticed, one of the most challenging scenes is those with high contrast between shadowy and bright regions. Each optical device, including the human eye, has a dynamic range. A dynamic range is a difference measured in EV units between the darkest and brightest part of the image that shows detail. Increase of one EV unit represents a situation where the amount of light is doubled. A human eye and a modern DSLR camera sensor have a dynamic range of approx. 14 EV (called “stops”). It means that we can double the amount of light 14 times between the brightest and darkest part of the image and still see detail.<br>We can control which part of the scene is covered by our EV range by adjusting the shutter speed, aperture or ISO value of the sensor. We, however, cannot increase this range."]},{"cell_type":"markdown","metadata":{"id":"51iDuQX8O_FP"},"source":["## What is HDR?\r\n","HDR, High Dynamic Range, is a photographing technique, in which a set of images is made with different camera setups. Each photograph covers a limited dynamic range, but a combination of the photographs covers a higher dynamic range resulting in an HDR photograph.<br>Of course, the display or printout also has a limited dynamic range, thus a mapping from the wider to the narrower dynamic range has to be done. This is referred to as “tone mapping”.<br>For an exemplary usage of the HDR technique, please take a look at the example below.\r\n","\r\n","<center><img src='https://drive.google.com/uc?export=view&id=1a6b01LPYeYMQnNGcYSUY62Zhp69RV0QI' width=\"600\"/>\r\n","<p>HDR input images. Note, that it was impossible to get details both on the bright (sky) and dark (shadows) areas in any single photo.</p><br><br>\r\n","<img src='https://drive.google.com/uc?export=view&id=1Q3sgo3W0GjGu4i9_4os3qFj5-we1HKw4' width=\"600\"/>\r\n","<p>HDR result. Notice, that it looks unnatural, as it shows a higher dynamic range than a human eye is able to process.</p></center>"]},{"cell_type":"markdown","metadata":{"id":"YLDvMx_MkDfR"},"source":["## High Dynamic Range (HDR) exercise\n","Here you can see how HDR works with examples. In exercise used 3 different HDR methods:\n","* Debevec\n","* Robertson\n","* Martens\n","Each of them you can check with 3 images taken with different exposure time. \n","\n","Here you can see oryginal taken pictures with average time of exposure. Below them, choose from drop-down menu picture you would like to see after HDR conversion.\n","<table style=\"width:80%\">\n","  <tr>\n","    <th style=\"text-align:center\">Yosemite</th>\n","    <th style=\"text-align:center\">Garden</th>\n","    <th style=\"text-align:center\">Mount</th>\n","  </tr>\n","  <tr>\n","    <td><img src=\"https://drive.google.com/uc?export=view&id=1NothOGCG0HYhn6iyAo2w2bbPnO63zvma\"/></td>\n","    <td><img src=\"https://drive.google.com/uc?export=view&id=15SvrRxCbOHYHeLQ8t-b0jw6RK2IRQVSX\"/></td>\n","    <td><img src=\"https://drive.google.com/uc?export=view&id=1ITqflKJ3T0I2BPcRQAIo-H-d_M46cjKh\"/></td></td>\n","  </tr>\n","</table>\n"]},{"cell_type":"code","metadata":{"id":"170rQRYbkDfY"},"source":["#@title Exercise  - run cell and experiment with pre-loaded images { display-mode: \"form\" }\r\n","interact(hdr, image=['yosemite','garden','mount'], hdr_type=['robertson','mertens','debevec']);"],"execution_count":null,"outputs":[]}]}